---
title: Pesquisa Visual Computacional aplicativos para a realidade misturada Workshop de headsets em CVPR 2019
description: Visão geral e programação dos aplicativos Pesquisa Visual Computacional para o workshop sobre headsets de realidade misturada, a ser entregue na conferência CVPR em junho de 2019.
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: evento, modo de pesquisa, cvpr, pesquisa Visual computacional, Research HoloLens
ms.openlocfilehash: 7768f7a473e4bd98f4a2868161dd3d365bc94fc5c2e5f006f382046b680749a3
ms.sourcegitcommit: a1c086aa83d381129e62f9d8942f0fc889ffcab0
ms.translationtype: MT
ms.contentlocale: pt-BR
ms.lasthandoff: 08/05/2021
ms.locfileid: "115195618"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a>Pesquisa Visual Computacional aplicativos para headsets de realidade misturada

Organizado em conjunto com [CVPR 2019](https://cvpr2019.thecvf.com/)

Praia (AC) longa

17 de junho de 2019 (à tarde) – Hyatt Regency F


## <a name="organizers"></a>Organizadores
* Pollefeys de hipermarca
* Federica Bogo
* Johannes Schönberger
* Osman Ulusoy

## <a name="overview"></a>Visão geral

![Imagem do separador](images/cvpr2019_teaser2.jpg)

headsets de realidade misturada, como os Microsoft HoloLens estão se tornando plataformas poderosas para desenvolver aplicativos de pesquisa visual computacional. HoloLens O modo de pesquisa habilita a pesquisa Visual computacional no dispositivo fornecendo acesso a todos os fluxos de sensor de imagem bruta, incluindo profundidade e IR. Como o modo de pesquisa agora está disponível desde 2018 de maio, estamos começando a ver várias demonstrações interessantes e aplicativos que estão sendo desenvolvidos para HoloLens. 

O objetivo deste workshop é reunir alunos e pesquisadores interessados na visão computacional de aplicativos de realidade misturada. O workshop fornecerá um local para compartilhar demonstrações e aplicativos e aprenderá uns com os outros para criar ou portar aplicativos para a realidade misturada. 

Incentivamos envios sobre os tópicos de reconhecimento de objetos (ego), mão e rastreamento de usuários, reconhecimento de atividades, térrea, reconstrução de 3D, compreensão de cena, localização baseada em sensor, navegação e muito mais.

## <a name="paper-submission"></a>Envio de papel
* Prazo de envio de papel: 17 de maio
* Notificação aos autores: 24 de maio

Os envios de papel devem usar o modelo CVPR e são limitados a 4 páginas mais referências. Além disso, incentivamos os autores a enviar um vídeo mostrando seus aplicativos.
Observe que os envios de trabalhos publicados anteriormente são permitidos (incluindo o trabalho aceito para a conferência principal do CVPR 2019). 

Os envios podem ser carregados para o CMT: https://cmt3.research.microsoft.com/CVFORMR2019

Um subconjunto de documentos será selecionado para apresentação oral no workshop. No entanto, recomendamos enfaticamente que todos os autores apresentem seu trabalho durante a sessão de demonstração.


## <a name="schedule"></a>Agenda
* 13:30-13:45: Bem-vindo e abrindo comentários.
* 13:45-14:15: **palestras**: Prof. Marc POLLEFEYS, ETH Zurique/Microsoft. Título: egocentric Pesquisa Visual Computacional em HoloLens.
* 14:15-14:45: **palestras**: Prof. Kris Kitani, Carnegie Mellon University. Título: atividade egocentric e gerar previsão.
* 14:45-15:15: **palestras**: Dr. Yang Liu, Instituto de tecnologia da Califórnia. Título: capacitando um assistente cognitiva para o cego com realidade aumentada.
* 15:15-16:15: intervalo de café e demonstrações.
* 16:15-16:45: **palestras**: Prof. Kristen Grauman, Universidade do Texas em Austin/Facebook ai Research. Título: interação do objeto humano no vídeo da primeira pessoa.
* 16:45-17:15: apresentações verbais:
    * O registro fez navegação orthopedic fácil e autônoma com HoloLens. F. Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. Farshad, P. Furnstahl.
    * Learning estéreo ao percorrer um HoloLens. H. Zhan, Y. Pekelny, O. Ulusoy.
* 17:15-17:30: comentários finais.
